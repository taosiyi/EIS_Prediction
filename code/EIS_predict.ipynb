{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e185143a",
   "metadata": {},
   "source": [
    "# Model\n",
    "will add after accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiFeatureTransformer(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5338a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c57c4",
   "metadata": {},
   "source": [
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8022bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "def extract_file_key(file_name):\n",
    "    match = re.search(r'(rest|eis)_(\\d+)_cap_([\\d.]+)\\.pkl', file_name)\n",
    "    if match:\n",
    "        file_type = match.group(1)\n",
    "        num = int(match.group(2))\n",
    "        cap = float(match.group(3))\n",
    "        return (num, cap, file_type)\n",
    "    return None\n",
    "\n",
    "def train_tensor_build(directory_path, scaler_path):\n",
    "    scaler_input = StandardScaler()\n",
    "    scaler_output = StandardScaler()\n",
    "    rest_len = 14   # change for different case\n",
    "    eis_len = 107   # change for different case\n",
    "\n",
    "    input_tensors = []\n",
    "    output_tensors = []\n",
    "    input_data_list = []\n",
    "    output_data_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        file_pairs = {}\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_key = extract_file_key(file)\n",
    "            if not file_key:\n",
    "                continue\n",
    "            num, cap, file_type = file_key\n",
    "            key = (num, cap)\n",
    "            if key not in file_pairs:\n",
    "                file_pairs[key] = {'input': None, 'output': None}\n",
    "            if file_type == 'rest':\n",
    "                file_pairs[key]['input'] = file_path\n",
    "            elif file_type == 'eis':\n",
    "                file_pairs[key]['output'] = file_path\n",
    "\n",
    "        for key in sorted(file_pairs.keys(), key=lambda x: (x[0],x[1])):\n",
    "            pair = file_pairs[key]\n",
    "            if not pair['input'] or not pair['output']:\n",
    "                continue\n",
    "\n",
    "            # input --------------------------------------------------\n",
    "            df_input = pd.read_pickle(pair['input'])\n",
    "            df_input.loc[df_input['time'] < 0] = np.nan\n",
    "            df_input.fillna(0, inplace=True)\n",
    "\n",
    "            if len(df_input) < rest_len:\n",
    "                pad_rows = rest_len - len(df_input)\n",
    "                padding = pd.DataFrame(np.zeros((pad_rows, df_input.shape[1])), \n",
    "                                    columns=df_input.columns)\n",
    "                df_input = pd.concat([df_input, padding], ignore_index=True)\n",
    "            elif len(df_input) > rest_len:\n",
    "                df_input = df_input.iloc[:rest_len]\n",
    "\n",
    "            input_segment = df_input[['Ecell/V', '<I>/mA', 'Temperature/°C', 'time']].values\n",
    "            input_data_list.append(input_segment)\n",
    "\n",
    "            # output --------------------------------------------------\n",
    "            df_output = pd.read_pickle(pair['output'])\n",
    "            if len(df_output) < eis_len:\n",
    "                pad_rows = eis_len - len(df_output)\n",
    "                padding = pd.DataFrame(np.zeros((pad_rows, df_output.shape[1])),\n",
    "                                    columns=df_output.columns)\n",
    "                df_output = pd.concat([df_output, padding], ignore_index=True)\n",
    "            elif len(df_output) > eis_len:\n",
    "                df_output = df_output.iloc[:eis_len]\n",
    "            if {'|Z|/Ohm', 'Phase(Z)/deg'}.issubset(df_output.columns):\n",
    "                output_segment = df_output[['|Z|/Ohm', 'Phase(Z)/deg']].iloc[1:eis_len].values\n",
    "                output_data_list.append(output_segment)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # normalization --------------------------------------------------\n",
    "    if input_data_list:\n",
    "        input_data_combined = np.vstack(input_data_list)\n",
    "        input_data_normalized = scaler_input.fit_transform(input_data_combined)\n",
    "        os.makedirs(scaler_path, exist_ok=True)\n",
    "        joblib.dump(scaler_input, os.path.join(scaler_path, \"scaler_input.pth\"))\n",
    "\n",
    "        split_indices = np.cumsum([len(arr) for arr in input_data_list])[:-1]\n",
    "        input_data_split = np.split(input_data_normalized, split_indices)\n",
    "        for segment in input_data_split:\n",
    "            tensor = torch.tensor(segment, dtype=torch.float32).view(-1, 4)  # 4 input features: V I T t\n",
    "            input_tensors.append(tensor)\n",
    "\n",
    "    if output_data_list:\n",
    "        output_data_combined = np.vstack(output_data_list)\n",
    "        output_data_normalized = scaler_output.fit_transform(output_data_combined)\n",
    "        joblib.dump(scaler_output, os.path.join(scaler_path, \"scaler_output.pth\"))\n",
    "\n",
    "        split_indices = np.cumsum([len(arr) for arr in output_data_list])[:-1]\n",
    "        output_data_split = np.split(output_data_normalized, split_indices)\n",
    "        for segment in output_data_split:\n",
    "            tensor = torch.tensor(segment, dtype=torch.float32).view(-1, 2)  # 2 output features: |Z| θ\n",
    "            output_tensors.append(tensor)\n",
    "\n",
    "    if input_tensors and output_tensors:\n",
    "        input_tensor = torch.stack(input_tensors).permute(1, 0, 2)  # [14, N, 4]\n",
    "        output_tensor = torch.stack(output_tensors).permute(1, 0, 2)  # [107, N, 2]\n",
    "        return input_tensor, output_tensor\n",
    "    else:\n",
    "        raise ValueError(\"no valid data for tensor\")\n",
    "\n",
    "def create_dataloaders(input_tensor, output_tensor, batch_size=32, train_ratio=0.8, shuffle=True):\n",
    "    input_data = input_tensor.permute(1, 0, 2)\n",
    "    output_data = output_tensor.permute(1, 0, 2)\n",
    "    dataset = TensorDataset(input_data, output_data)\n",
    "\n",
    "    total_samples = len(dataset)\n",
    "    train_size = int(train_ratio * total_samples)\n",
    "    val_size = total_samples - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad782b29",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6acb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def train_model(directory_path, model_path):\n",
    "    epochs = 1000\n",
    "    input_dim = 4\n",
    "    output_dim = 2\n",
    "    seq_len_in = 14     # change for different case\n",
    "    seq_len_out = 106   # change for different case\n",
    "    d_model = 256\n",
    "    nhead = 4\n",
    "    num_layers = 6\n",
    "\n",
    "    model = MultiFeatureTransformer(\n",
    "        input_dim, output_dim, seq_len_in, seq_len_out, \n",
    "        d_model, nhead, num_layers\n",
    "        ).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    input_tensor, output_tensor = train_tensor_build(directory_path, model_path)\n",
    "    train_loader, val_loader = create_dataloaders(input_tensor, output_tensor)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch, (src, tgt) in enumerate(train_loader):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output= model(src)\n",
    "            loss = criterion(output, tgt)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:                \n",
    "                src = src.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "                output = model(src)\n",
    "                val_loss += criterion(output, tgt).item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "        # save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_path = os.path.join(model_path, \"best_model.pth\")\n",
    "            torch.save(model.to('cpu').state_dict(), best_model_path)  # save on CPU\n",
    "            model.to(device)  # train on GPU\n",
    "            print(f\"✅ Saved best model at epoch {epoch + 1}\")\n",
    "    \n",
    "    # save final model\n",
    "    torch.save(model.to('cpu').state_dict(), os.path.join(model_path, \"final_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e9911",
   "metadata": {},
   "source": [
    "## main_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc71e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r\"data_120s\\train\"\n",
    "model_path = \"model\"\n",
    "\n",
    "train_model(directory_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e8900",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d44600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"enable_nested_tensor is True, but self.use_nested_tensor is False\")\n",
    "\n",
    "def test_result_get_single(model_path, cell_path, input_file):\n",
    "    # 数据准备\n",
    "    scaler_path = os.path.join(model_path, \"scaler_input.pth\")\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    rest_len = 14\n",
    "    input_tensor = None\n",
    "    file_path = os.path.join(cell_path, input_file)\n",
    "    df = pd.read_pickle(file_path)\n",
    "    segment = df[['Ecell/V', '<I>/mA', 'Temperature/°C', 'time']].iloc[:rest_len].values\n",
    "    input_scaled = scaler.transform(segment)\n",
    "    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    input_dim = 4\n",
    "    output_dim = 2\n",
    "    seq_len_in = 14     # change for different case\n",
    "    seq_len_out = 106   # change for different case\n",
    "    d_model = 256\n",
    "    nhead = 4\n",
    "    num_layers = 6\n",
    "\n",
    "    model = MultiFeatureTransformer(\n",
    "        input_dim, output_dim, seq_len_in, seq_len_out, \n",
    "        d_model, nhead, num_layers\n",
    "        ).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_path, \"best_model.pth\")))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = input_tensor.to(device)\n",
    "        output = model(src)\n",
    "    \n",
    "    scaler_output = joblib.load(os.path.join(model_path, \"scaler_output.pth\"))\n",
    "    output = output.cpu().numpy()\n",
    "    output = scaler_output.inverse_transform(output.reshape(-1, 2)).reshape(output.shape)\n",
    "    prediction = output\n",
    "    prediction = output.squeeze(0)\n",
    "    return prediction\n",
    "\n",
    "def save_single_eis_results_as_pkl(prediction, result_file, cell_path, output_file):\n",
    "\n",
    "    eis_path = os.path.join(cell_path, output_file)\n",
    "    df = pd.read_pickle(eis_path)\n",
    "    freq_meas = df['freq/Hz'].values\n",
    "    meas_real = df['|Z|/Ohm'].values\n",
    "    meas_imag = df['Phase(Z)/deg'].values\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'freq': freq_meas[1:],\n",
    "        'pred_mag': prediction[:, 0],\n",
    "        'pred_ph': prediction[:, 1],\n",
    "        'meas_mag': meas_real[1:],\n",
    "        'meas_ph': meas_imag[1:]\n",
    "    })\n",
    "\n",
    "    directory = os.path.dirname(result_file)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    with open(result_file, 'wb') as f:\n",
    "        pickle.dump(result_df, f)\n",
    "\n",
    "def extract_number_eis(file_name):\n",
    "    match = re.search(r'eis_(\\d+)_', file_name)\n",
    "    return int(match.group(1)) if match else -1  # Return -1 if no match found\n",
    "\n",
    "def extract_number_rest(file_name):\n",
    "    match = re.search(r'rest_(\\d+)_', file_name)\n",
    "    return int(match.group(1)) if match else -1  # Return -1 if no match found\n",
    "\n",
    "def extract_info(file_name):\n",
    "    eis_match = re.search(r'eis_(\\d+)_', file_name)\n",
    "    cap_match = re.search(r'cap_(\\d+\\.?\\d*)\\.pkl', file_name)\n",
    "    \n",
    "    eis_number = int(eis_match.group(1)) if eis_match else None\n",
    "    cap_value = float(cap_match.group(1)) if cap_match else None\n",
    "    \n",
    "    return eis_number, cap_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94559275",
   "metadata": {},
   "source": [
    "## save predicted EIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"data_120s\\test\"\n",
    "model_path = \"model\"\n",
    "\n",
    "start_time = time.time()\n",
    "for cell in os.listdir(data_path):\n",
    "    cell_path = os.path.join(data_path, cell)\n",
    "\n",
    "    if os.path.isdir(cell_path):\n",
    "        print(f\"Running test for cell: {cell}\")\n",
    "        cell_path = rf\"{data_path}\\{cell}\"\n",
    "        \n",
    "        input_files = [f for f in os.listdir(cell_path) if f.endswith(\".pkl\") and 'rest_' in f]\n",
    "        sorted_input_files = sorted(input_files, key=lambda x: extract_number_rest(x))\n",
    "\n",
    "        output_files = [f for f in os.listdir(cell_path) if f.endswith(\".pkl\") and 'eis_' in f]\n",
    "        sorted_output_files = sorted(output_files, key=lambda x: extract_number_eis(x))\n",
    "\n",
    "        for input_file, output_file in zip(sorted_input_files,sorted_output_files):\n",
    "            test_preds = test_result_get_single(model_path, cell_path, input_file)\n",
    "\n",
    "            eis_number, cap_value = extract_info(output_file)\n",
    "            result_file = rf\"result\\{cell}\\predicted_eis_{eis_number}_cap_{cap_value}.pkl\"\n",
    "\n",
    "            save_single_eis_results_as_pkl(test_preds, result_file, cell_path, output_file)\n",
    "\n",
    "end_time = time.time()\n",
    "test_duration = end_time - start_time\n",
    "print(f\"Time: {test_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce669a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
